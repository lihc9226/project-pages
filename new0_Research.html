---
layout: page
title: "Research"
description: "Description of Our research"
header-img: "img/home-bg-1.jpg"
---

Welcome to our channel! Here are some brief introduction of our research:

<font size="10px" color="red">Activity Recognition with Sensor Network</font><br><br>

<html>
 <head></head>
 <body>
   <p class="col-md-6contact-left"> 
   <p style="text-align:center"><iframe width="300" height="200" src="https://www.youtube.com/embed/4_RgzVeb4oQ" frameborder="0" align="left" allow="autoplay; encrypted-media" allowfullscreen></iframe></p> 
   </p>
 </body>
</html>
<br><br>

<strong> Region-based Activity Recognition Using Conditional GAN</strong>
<br><br>
We present a method for activity recognition that first estimates the activity performer's location and uses it with input data
for activity recognition. Existing approaches directly take video frames or entire video for feature extraction and 
recognition, and treat the classifier as a black box. Our method first locates the activities in each input video frame by 
generating an activity mask using a conditional generative adversarial network (cGAN). The generated mask is appended to 
color channels of input images and fed into a VGG-LSTM network for activity recognition. To test our system, we produced 
two datasets with manually created masks, one containing Olympic sports activities and the other containing trauma 
resuscitation activities. Our system makes activity prediction for each video frame and achieves performance comparable to
the state-of-the-art systems while simultaneously outlining the location of the activity. We show how the generated masks 
facilitate the learning of features that are representative of the activity rather than accidental surrounding information.

<strong>Activity Recognition for Medical Teamwork Based on Passive RFID</strong>
<br><br>
We describe a novel and practical activity recognition system for dynamic and complex medical settings using only passive 
RFID technology. Our activity recognition approach is based on the use of objects that are specific for a given activity. 
The object-use status is detected from RFID data and the activities are predicted from the statuses of use of different 
objects. We tagged 10 objects in a trauma room of an emergency department and recorded RFID data for 10 actual trauma 
resuscitation. More than 20,000 seconds of data were collected and used for analysis. The system achieved a 96% overall 
accuracy with a 0.74 F-score for detecting use of 10 common resuscitation objects and 95% accuracy with a 0.30 F-Score 
for activity recognition of 10 medical activities.  
<b><a href="http://www.google.com/url?q=http%3A%2F%2Fieeexplore.ieee.org%2Fiel7%2F7483866%2F7487991%2F07488002.pdf&sa=D&sntz=1&usg=AFQjCNEJv-9lclna2LUmcWn72ufm0HsHNw">Paper</a></b>.

<br><br><br><strong>Deep Neural Network for RFID-Based Activity Recognition</strong>
<br><br>
We propose a Deep Neural Network (DNN) structure for RFIDbased activity recognition. RFID data collected from several 
reader antennas with overlapping coverage have potential spatiotemporal relationships that can be used for object tracking.
We augmented the standard fully-connected DNN structure with additional pooling layers to extract the most representative 
features. For model training and testing, we used RFID data from 12 tagged objects collected during 25 actual trauma 
resuscitations. Our results showed 76% recognition micro-accuracy for 7 resuscitation activities and 85% average 
micro-accuracy for 5 resuscitation phases, which is similar to existing system that, however, require the user to wear 
an RFID antenna.  
<b><a href="http://www.google.com/url?q=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2987355&sa=D&sntz=1&usg=AFQjCNGWp_SFZuWjee5_6qE6gx7zwmoFGQ">Paper</a></b>.

<br><br><br><strong>Deep Learning for RFID-Based Activity Recognition</strong>
<br><br>
We present a system for activity recognition from passive RFID data using a deep convolutional neural network. We directly 
feed the RFID data into a deep convolutional neural network for activity recognition instead of selecting features and 
using a cascade structure that first detects object use from RFID data followed by predicting the activity. Because our 
system treats activity recognition as a multi-class classification problem, it is scalable for applications with large 
number of activity classes. We tested our system using RFID data collected in a trauma room, including 14 hours of RFID 
data from 16 actual trauma resuscitations. Our system outperformed existing systems developed for activity recognition and 
achieved similar performance with process-phase detection as systems that require wearable sensors or manually-generated 
input. We also analyzed the strengths and limitations of our current deep learning architecture for activity recognition 
from RFID data.
<b><a href="http://www.google.com/url?q=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2994569&sa=D&sntz=1&usg=AFQjCNE2s8gl_0UrJSbkXlShbB1Clnz-vA">Paper</a></b>. 

<br><br><br><strong>Concurrent Activity Recognition with Multimodal CNN-LSTM Structure</strong>
<br><br>
We  introduce  a  system  that  recognizes  concurrent  activities  from real-world data captured by multiple sensors of 
different types. The recognition  is  achieved  in  two  steps.  First,  we  extract  spatial  and temporal features from 
the multimodal data. We feed each data type into  a  convolutional  neural  network  that  extracts  spatial  features,
followed  by  a  long-short  term  memory  network  that  extracts temporal information in the sensory data. The extracted 
features are then  fused  for  decision  making  in  the  second  step.  Second,  we achieve concurrent activity 
recognition with a single classifier that encodes a binary output vector in which elements indicate whether the  
corresponding  activity  types  are  currently  in  progress.  We tested  our  system  with  three  datasets  from  
different  domains recorded   using   different   sensors   and   achieved   performance comparable  to  existing  
systems  designed  specifically  for those domains. Our system is the first to address the concurrent activity recognition 
with multi-sensory data using a single model, which is scalable, simple to train and easy to deploy. 
<b><a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1702.01638&sa=D&sntz=1&usg=AFQjCNFwTHchJUaBceYJYSEJ150nXOBdKw">Preprint Paper</a></b>.


<br><br>


<html>
 <head></head>
 <body>
  
 <br><br>
 <p><iframe width="50%" height="500px" src="https://www.youtube.com/embed/p3MzFN3i_eU" name="contentIFrame" frameborder="0" scrolling="yes" style="float: left;"></iframe>

 <iframe width="50%" height="500px" src="https://www.youtube.com/embed/Pa1Lbbx66Eg" name="contentIFrame" id="contentIFrame" frameborder="0" scrolling="yes" style="float: right;"></iframe></p>
 
 </body>
</html>
